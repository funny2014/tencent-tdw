query: EXPLAIN EXTENDED SELECT explode(array(1,2,3)) AS myCol FROM src LIMIT 3
ABSTRACT SYNTAX TREE:
  (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TAB src))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_FUNCTION explode (TOK_FUNCTION array 1 2 3)) myCol)) (TOK_LIMIT 3)))

STAGE DEPENDENCIES:
  Stage-1
    type:root stage;
  Stage-0
    type:root stage;

STAGE PLANS:
  Stage: Stage-1
    Map Reduce
      Alias -> Map Operator Tree:
        default_db/src 
          Operator:          TableScan
            alias: default_db/src
            Operator:            Select Operator
              expressions:
                    expr: array(1,2,3)
                    type: array<int>
              outputColumnNames: _col0
              Operator:              UDTF Operator
                function name: explode
                Operator:                Limit
                  Operator:                  File Output Operator
                    compressed: false
                    GlobalTableId: 0
                    directory: file:/data/tdwadmin/tdwv1.0R020D004/qe_br_020_ng_new/build/ql/tmp/586198881/10001
                    table:
                      table descs
                        input format: org.apache.hadoop.mapred.TextInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                        properties:
                          columns col
                          serialization.format 1
                          columns.types int
      Needs Tagging: false
      Path -> Alias:
        file:/data/tdwadmin/tdwv1.0R020D004/qe_br_020_ng_new/build/ql/test/data/warehouse/default_db/src [default_db/src]
      Path -> Partition:
        file:/data/tdwadmin/tdwv1.0R020D004/qe_br_020_ng_new/build/ql/test/data/warehouse/default_db/src 
          Partition
          
            table descs
              input format: org.apache.hadoop.mapred.TextInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
              properties:
                name src
                columns.types string:string
                serialization.ddl struct src { string key, string value}
                serialization.format 1
                columns key,value
                bucket_count -1
                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                file.inputformat org.apache.hadoop.mapred.TextInputFormat
                file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                location file:/data/tdwadmin/tdwv1.0R020D004/qe_br_020_ng_new/build/ql/test/data/warehouse/default_db/src
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: src

  Stage: Stage-0
    Fetch Operator
      limit: 3

query: EXPLAIN EXTENDED SELECT a.myCol, count(1) FROM (SELECT explode(array(1,2,3)) AS myCol FROM src LIMIT 3) a GROUP BY a.myCol
ABSTRACT SYNTAX TREE:
  (TOK_QUERY (TOK_FROM (TOK_SUBQUERY (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TAB src))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_FUNCTION explode (TOK_FUNCTION array 1 2 3)) myCol)) (TOK_LIMIT 3))) a)) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (. (TOK_TABLE_OR_COL a) myCol)) (TOK_SELEXPR (TOK_FUNCTION count 1))) (TOK_GROUPBY (. (TOK_TABLE_OR_COL a) myCol))))

STAGE DEPENDENCIES:
  Stage-1
    type:root stage;
  Stage-2
    type:;depends on:Stage-1;
  Stage-0
    type:root stage;

STAGE PLANS:
  Stage: Stage-1
    Map Reduce
      Alias -> Map Operator Tree:
        a:default_db/src 
          Operator:          TableScan
            alias: default_db/src
            Operator:            Select Operator
              expressions:
                    expr: array(1,2,3)
                    type: array<int>
              outputColumnNames: _col0
              Operator:              UDTF Operator
                function name: explode
                Operator:                Limit
                  Operator:                  Reduce Output Operator
                    key serialize infos:
                      table descs
                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                        properties:
                          columns 
                          serialization.sort.order 
                          columns.types 
                    sort order: 
                    output value names: _col0
                    tag: -1
                    value expressions:
                          expr: col
                          type: int
      Needs Tagging: false
      Path -> Alias:
        file:/data/tdwadmin/tdwv1.0R020D004/qe_br_020_ng_new/build/ql/test/data/warehouse/default_db/src [a:default_db/src]
      Path -> Partition:
        file:/data/tdwadmin/tdwv1.0R020D004/qe_br_020_ng_new/build/ql/test/data/warehouse/default_db/src 
          Partition
          
            table descs
              input format: org.apache.hadoop.mapred.TextInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
              properties:
                name src
                columns.types string:string
                serialization.ddl struct src { string key, string value}
                serialization.format 1
                columns key,value
                bucket_count -1
                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                file.inputformat org.apache.hadoop.mapred.TextInputFormat
                file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                location file:/data/tdwadmin/tdwv1.0R020D004/qe_br_020_ng_new/build/ql/test/data/warehouse/default_db/src
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: src
      Reduce Operator Tree:
        Operator:        Extract
          Operator:          Limit
            Operator:            Select Operator
              expressions:
                    expr: _col0
                    type: int
              outputColumnNames: _col0
              Operator:              Group By Operator
                aggregations:
                      expr: count(1)
                keys:
                      expr: _col0
                      type: int
                mode: hash
                outputColumnNames: _col0, _col1
                UseNewGroupBy: true
                Operator:                File Output Operator
                  compressed: false
                  GlobalTableId: 0
                  directory: file:/data/tdwadmin/tdwv1.0R020D004/qe_br_020_ng_new/build/ql/tmp/1275983419/10002
                  table:
                    table descs
                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                      properties:
                        columns _col0,_col1
                        columns.types int,uniontype<struct<_col0:bigint>>
                        escape.delim \

  Stage: Stage-2
    Map Reduce
      Alias -> Map Operator Tree:
        file:/data/tdwadmin/tdwv1.0R020D004/qe_br_020_ng_new/build/ql/tmp/1275983419/10002 
            Operator:            Reduce Output Operator
              key expressions:
                    expr: _col0
                    type: int
              key serialize infos:
                table descs
                  input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                  output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                  properties:
                    columns _col0,_col1
                    serialization.sort.order ++
                    columns.types int,uniontype<struct<_col0:bigint>>
              sort order: ++
              output key names: _col0, _col1
              tag: -1
      Needs Tagging: false
      Path -> Alias:
        file:/data/tdwadmin/tdwv1.0R020D004/qe_br_020_ng_new/build/ql/tmp/1275983419/10002 [file:/data/tdwadmin/tdwv1.0R020D004/qe_br_020_ng_new/build/ql/tmp/1275983419/10002]
      Path -> Partition:
        file:/data/tdwadmin/tdwv1.0R020D004/qe_br_020_ng_new/build/ql/tmp/1275983419/10002 
          Partition
          
            table descs
              input format: org.apache.hadoop.mapred.SequenceFileInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
              properties:
                columns _col0,_col1
                columns.types int,uniontype<struct<_col0:bigint>>
                escape.delim \
      Reduce Operator Tree:
        Operator:        Group By Operator
          aggregations:
                expr: count(KEY._col1:0._col0)
          keys:
                expr: KEY._col0
                type: int
          mode: mergepartial
          outputColumnNames: _col0, _col1
          UseNewGroupBy: true
          Operator:          Select Operator
            expressions:
                  expr: _col0
                  type: int
                  expr: _col1
                  type: bigint
            outputColumnNames: _col0, _col1
            Operator:            File Output Operator
              compressed: false
              GlobalTableId: 0
              directory: file:/data/tdwadmin/tdwv1.0R020D004/qe_br_020_ng_new/build/ql/tmp/1275983419/10001
              table:
                table descs
                  input format: org.apache.hadoop.mapred.TextInputFormat
                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                  properties:
                    columns _col0,_col1
                    serialization.format 1
                    columns.types int:bigint

  Stage: Stage-0
    Fetch Operator
      limit: -1

query: SELECT explode(array(1,2,3)) AS myCol FROM src LIMIT 3
Output: file:/data/tdwadmin/tdwv1.0R020D004/qe_br_020_ng_new/build/ql/tmp/219564878/10000
1
2
3
query: SELECT explode(array(1,2,3)) AS (myCol) FROM src LIMIT 3
Output: file:/data/tdwadmin/tdwv1.0R020D004/qe_br_020_ng_new/build/ql/tmp/159027488/10000
1
2
3
query: SELECT a.myCol, count(1) FROM (SELECT explode(array(1,2,3)) AS myCol FROM src LIMIT 3) a GROUP BY a.myCol
Output: file:/data/tdwadmin/tdwv1.0R020D004/qe_br_020_ng_new/build/ql/tmp/1876314274/10000
1	1
2	1
3	1
