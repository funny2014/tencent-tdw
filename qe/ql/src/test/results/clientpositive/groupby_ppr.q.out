query: CREATE TABLE dest1(key STRING, c1 INT, c2 STRING) STORED AS TEXTFILE
query: EXPLAIN EXTENDED
FROM srcpart src
INSERT OVERWRITE TABLE dest1 
SELECT substr(src.key,1,1), count(DISTINCT substr(src.value,5)), concat(substr(src.key,1,1),sum(substr(src.value,5))) 
WHERE src.ds = '2008-04-08'
GROUP BY substr(src.key,1,1)
ABSTRACT SYNTAX TREE:
  (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TAB srcpart) src)) (TOK_INSERT (TOK_DESTINATION (TOK_TABDEST (TOK_TAB dest1))) (TOK_SELECT (TOK_SELEXPR (TOK_FUNCTION substr (. (TOK_TABLE_OR_COL src) key) 1 1)) (TOK_SELEXPR (TOK_FUNCTIONDI count (TOK_FUNCTION substr (. (TOK_TABLE_OR_COL src) value) 5))) (TOK_SELEXPR (TOK_FUNCTION concat (TOK_FUNCTION substr (. (TOK_TABLE_OR_COL src) key) 1 1) (TOK_FUNCTION sum (TOK_FUNCTION substr (. (TOK_TABLE_OR_COL src) value) 5))))) (TOK_WHERE (= (. (TOK_TABLE_OR_COL src) ds) '2008-04-08')) (TOK_GROUPBY (TOK_FUNCTION substr (. (TOK_TABLE_OR_COL src) key) 1 1))))

STAGE DEPENDENCIES:
  Stage-1
    type:root stage;
  Stage-0
    type:;depends on:Stage-1;

STAGE PLANS:
  Stage: Stage-1
    Map Reduce
      Alias -> Map Operator Tree:
        default_db/srcpart#src 
          Operator:          TableScan
            alias: default_db/srcpart#src
            Operator:            Filter Operator
              isSamplingPred: false
              predicate:
                  expr: (ds = '2008-04-08')
                  type: boolean
              Operator:              Select Operator
                expressions:
                      expr: key
                      type: string
                      expr: value
                      type: string
                outputColumnNames: key, value
                Operator:                Reduce Output Operator
                  key expressions:
                        expr: substr(key, 1, 1)
                        type: string
                        expr: substr(value, 5)
                        type: string
                  key serialize infos:
                    table descs
                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                      properties:
                        columns _col0,_col1
                        serialization.sort.order ++
                        columns.types string,uniontype<struct<_col0:string>>
                  sort order: ++
                  output key names: _col0, _col1
                  Map-reduce partition columns:
                        expr: substr(key, 1, 1)
                        type: string
                  tag: -1
      Needs Tagging: false
      Path -> Alias:
        file:/data/tdwadmin/tdwqev1.0R020/qe/build/ql/test/data/warehouse/default_db/srcpart/p0/sp1 [default_db/srcpart#src]
        file:/data/tdwadmin/tdwqev1.0R020/qe/build/ql/test/data/warehouse/default_db/srcpart/p0/sp2 [default_db/srcpart#src]
      Path -> Partition:
        file:/data/tdwadmin/tdwqev1.0R020/qe/build/ql/test/data/warehouse/default_db/srcpart/p0/sp1 
          Partition
          
            table descs
              input format: org.apache.hadoop.mapred.TextInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
              properties:
                name srcpart
                columns.types string:string:string:string
                serialization.ddl struct srcpart { string key, string value, string ds, string hr}
                serialization.format 1
                columns key,value,ds,hr
                bucket_count -1
                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                file.inputformat org.apache.hadoop.mapred.TextInputFormat
                file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                location file:/data/tdwadmin/tdwqev1.0R020/qe/build/ql/test/data/warehouse/default_db/srcpart
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: srcpart
        file:/data/tdwadmin/tdwqev1.0R020/qe/build/ql/test/data/warehouse/default_db/srcpart/p0/sp2 
          Partition
          
            table descs
              input format: org.apache.hadoop.mapred.TextInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
              properties:
                name srcpart
                columns.types string:string:string:string
                serialization.ddl struct srcpart { string key, string value, string ds, string hr}
                serialization.format 1
                columns key,value,ds,hr
                bucket_count -1
                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                file.inputformat org.apache.hadoop.mapred.TextInputFormat
                file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                location file:/data/tdwadmin/tdwqev1.0R020/qe/build/ql/test/data/warehouse/default_db/srcpart
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: srcpart
      Reduce Operator Tree:
        Operator:        Group By Operator
          aggregations:
                expr: count(DISTINCT KEY._col1:0._col0)
                expr: sum(KEY._col1:0._col0)
          keys:
                expr: KEY._col0
                type: string
          mode: complete
          outputColumnNames: _col0, _col1, _col2
          Operator:          Select Operator
            expressions:
                  expr: _col0
                  type: string
                  expr: _col1
                  type: bigint
                  expr: concat(_col0, _col2)
                  type: string
            outputColumnNames: _col0, _col1, _col2
            Operator:            Select Operator
              expressions:
                    expr: _col0
                    type: string
                    expr: UDFToInteger(_col1)
                    type: int
                    expr: _col2
                    type: string
              outputColumnNames: _col0, _col1, _col2
              Operator:              File Output Operator
                compressed: false
                GlobalTableId: 1
                directory: file:/data/tdwadmin/tdwqev1.0R020/qe/build/ql/tmp/1524989309/10000
                table:
                  table descs
                    input format: org.apache.hadoop.mapred.TextInputFormat
                    output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                    properties:
                      name dest1
                      columns.types string:int:string
                      serialization.ddl struct dest1 { string key, i32 c1, string c2}
                      serialization.format 1
                      columns key,c1,c2
                      bucket_count -1
                      serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                      file.inputformat org.apache.hadoop.mapred.TextInputFormat
                      file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                      location file:/data/tdwadmin/tdwqev1.0R020/qe/build/ql/test/data/warehouse/default_db/dest1
                      db default_db
                    serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                    name: dest1

  Stage: Stage-0
    Move Operator
      tables:
          replace: true
          source: file:/data/tdwadmin/tdwqev1.0R020/qe/build/ql/tmp/1524989309/10000
          table:
            table descs
              input format: org.apache.hadoop.mapred.TextInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
              properties:
                name dest1
                columns.types string:int:string
                serialization.ddl struct dest1 { string key, i32 c1, string c2}
                serialization.format 1
                columns key,c1,c2
                bucket_count -1
                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                file.inputformat org.apache.hadoop.mapred.TextInputFormat
                file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                location file:/data/tdwadmin/tdwqev1.0R020/qe/build/ql/test/data/warehouse/default_db/dest1
                db default_db
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: dest1
          tmp directory: file:/data/tdwadmin/tdwqev1.0R020/qe/build/ql/tmp/1524989309/10001

query: FROM srcpart src
INSERT OVERWRITE TABLE dest1 
SELECT substr(src.key,1,1), count(DISTINCT substr(src.value,5)), concat(substr(src.key,1,1),sum(substr(src.value,5))) 
WHERE src.ds = '2008-04-08'
GROUP BY substr(src.key,1,1)
Output: default_db/dest1
query: SELECT dest1.* FROM dest1
Input: default_db/dest1
Output: file:/data/tdwadmin/tdwqev1.0R020/qe/build/ql/tmp/1817557663/10000
0	1	00.0
1	49	19353.0
2	41	212775.0
3	39	316119.0
4	43	423012.0
5	4	5271.0
6	3	6198.0
7	2	7221.0
8	7	8515.0
9	5	9569.0
query: SELECT dest1.* FROM dest1
query: SELECT dest1.* FROM dest1
query: SELECT dest1.* FROM dest1
query: SELECT dest1.* FROM dest1
query: SELECT dest1.* FROM dest1
query: SELECT dest1.* FROM dest1
query: SELECT dest1.* FROM dest1
query: SELECT dest1.* FROM dest1
